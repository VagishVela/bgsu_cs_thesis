{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G54aSNwBQyyG",
    "outputId": "eaadddb8-f2c4-4da0-8c6b-0a15380010d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gitpod/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pubmed_parser as pp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "from csv import DictWriter\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ET9nFBlZHdrr"
   },
   "outputs": [],
   "source": [
    "path_xml = pp.list_xml_path('../../data/raw') # list all xml paths under directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first set of data from the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DU6Uu3ppHtVs"
   },
   "outputs": [],
   "source": [
    "pubmed_dict = pp.parse_medline_xml(path_xml[0]) # dictionary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words_from_corpus(corpus, stop_words=[], stemming=False):\n",
    "    sno_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    bag_of_words = []\n",
    "    for doc in corpus:\n",
    "        docWords = []\n",
    "        for term in nltk.word_tokenize(doc['abstract']):\n",
    "            if term.lower() not in stop_words:\n",
    "                if stemming:\n",
    "                    docWords.append(sno_stemmer.stem(term))\n",
    "                else:\n",
    "                    docWords.append(term)\n",
    "        bag_of_words.append(\n",
    "            {\n",
    "                'nlm_unique_id': doc['nlm_unique_id'],\n",
    "                'bag_of_words': docWords\n",
    "            }\n",
    "        )\n",
    "    return bag_of_words\n",
    "\n",
    "def get_all_terms_from_corpus(bag_of_words_corpus):\n",
    "    all_terms = set()\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        all_terms.update(doc['bag_of_words'])\n",
    "    return list(all_terms)\n",
    "\n",
    "# This conversion to a dataframe causes some issues as the array isn't going through properly.\n",
    "def write_dict_to_json(dict, file_name):\n",
    "    f = open('../../data/processed/' + file_name + '.json', \"w\")\n",
    "    json.dump(dict, f)\n",
    "    f.close()\n",
    "\n",
    "def read_dict_from_json(file_name):\n",
    "    with open('../../data/processed/' + file_name + '.json') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def create_frequency_dict(all_terms, values):\n",
    "    return dict(zip(all_terms, [values for _ in all_terms]))\n",
    "\n",
    "def get_doc_frequencies(bag_of_words_corpus, all_terms):\n",
    "    documents_frequencies = create_frequency_dict(all_terms, 0)\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in list(uniq_tokens):\n",
    "            documents_frequencies[uniq_token] += 1\n",
    "\n",
    "    return documents_frequencies\n",
    "\n",
    "def get_term_frequencies(bag_of_words_corpus, terms):\n",
    "    terms_frequencies = create_frequency_dict(terms, [])\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in uniq_tokens:\n",
    "            frequency = { 'nlm_unique_id': doc['nlm_unique_id'], 'freq': doc['bag_of_words'].count(uniq_token) }\n",
    "            if terms_frequencies[uniq_token]:\n",
    "                terms_frequencies[uniq_token].append(frequency)\n",
    "            else:\n",
    "                terms_frequencies[uniq_token] = [frequency]\n",
    "\n",
    "    return terms_frequencies\n",
    "\n",
    "# Calculate the inverse document frequency\n",
    "def get_idf(df, total_document_count):\n",
    "    return math.log10(total_document_count/df)\n",
    "\n",
    "# Calculate the tf-idf weighting\n",
    "def get_tf_idf(tf, idf):\n",
    "    return tf * idf\n",
    "\n",
    "def get_doc_vector_lengths(abstract_bag_of_words, doc_frequencies, term_frequencies):\n",
    "    vectorDocLengths = dict(zip([_['nlm_unique_id'] for _ in abstract_bag_of_words], [0 for _ in abstract_bag_of_words]))\n",
    "    totalCorpusDocCount = len(vectorDocLengths)\n",
    "\n",
    "    for doc in abstract_bag_of_words:\n",
    "        selected_doc_id = doc['nlm_unique_id']\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        totalWf2 = 0\n",
    "        for uniq_token in list(uniq_tokens):\n",
    "            idf = get_idf(doc_frequencies[uniq_token], totalCorpusDocCount)\n",
    "            tf = [termFreq for termFreq in term_frequencies[uniq_token] if termFreq['nlm_unique_id'] == selected_doc_id][0]['freq']\n",
    "\n",
    "            weightingScheme = get_tf_idf(tf, idf)\n",
    "            wf2 = weightingScheme ** 2\n",
    "            totalWf2 += wf2\n",
    "\n",
    "        # Add document vector length to it's document\n",
    "        vectorDocLengths[selected_doc_id] = math.sqrt(totalWf2)\n",
    "\n",
    "    print (\"Generated document vector lengths for \" + str(len(vectorDocLengths)) + \" documents\")\n",
    "\n",
    "    return vectorDocLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis and output to json files as data is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_bag_of_words_from_corpus(pubmed_dict), 'abstract_bag_of_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_all_terms_from_corpus(read_dict_from_json('abstract_bag_of_words')), 'all_terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_doc_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'doc_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_term_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'term_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated document vector lengths for 2408 documents\n"
     ]
    }
   ],
   "source": [
    "write_dict_to_json(\n",
    "    get_doc_vector_lengths(\n",
    "        read_dict_from_json('abstract_bag_of_words'), \n",
    "        read_dict_from_json('doc_frequencies'),\n",
    "        read_dict_from_json('term_frequencies')\n",
    "    ), \n",
    "    'doc_vector_lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
