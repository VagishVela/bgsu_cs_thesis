{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G54aSNwBQyyG",
    "outputId": "eaadddb8-f2c4-4da0-8c6b-0a15380010d0"
   },
   "outputs": [],
   "source": [
    "import pubmed_parser as pp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from csv import DictWriter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ET9nFBlZHdrr"
   },
   "outputs": [],
   "source": [
    "path_xml = pp.list_xml_path('../data/raw') # list all xml paths under directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DU6Uu3ppHtVs"
   },
   "outputs": [],
   "source": [
    "pubmed_dict = pp.parse_medline_xml(path_xml[0]) # dictionary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "baewDehmIXrb",
    "outputId": "1ae1dc9c-6da1-4cc1-ed7f-8a89c586918a"
   },
   "outputs": [],
   "source": [
    "pubmed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words_from_corpus (corpus, stop_words=[], stemming=False):\n",
    "    sno_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    bag_of_words = []\n",
    "    for doc in corpus:\n",
    "        docWords = []\n",
    "        for term in nltk.word_tokenize(doc['abstract']):\n",
    "            if term.lower() not in stop_words:\n",
    "                if stemming:\n",
    "                    docWords.append(sno_stemmer.stem(term))\n",
    "                else:\n",
    "                    docWords.append(term)\n",
    "        bag_of_words.append(\n",
    "            {\n",
    "                'nlm_unique_id': doc['nlm_unique_id'],\n",
    "                'bag_of_words': docWords\n",
    "            }\n",
    "        )\n",
    "    return bag_of_words\n",
    "\n",
    "def get_all_terms_from_corpus(bag_of_words_corpus):\n",
    "    all_terms = set()\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        all_terms.update(doc['bag_of_words'])\n",
    "    return list(all_terms)\n",
    "\n",
    "# This conversion to a dataframe causes some issues as the array isn't going through properly.\n",
    "def write_dict_to_json(dict, file_name):\n",
    "    f = open('../data/processed/' + file_name + '.json', \"w\")\n",
    "    json.dump(dict, f)\n",
    "    f.close()\n",
    "\n",
    "def read_dict_from_json(file_name):\n",
    "    with open('../data/processed/' + file_name + '.json') as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_bag_of_words_from_corpus(pubmed_dict), 'abstract_bag_of_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_all_terms_from_corpus(read_dict_from_json('abstract_bag_of_words')), 'all_terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_dict(all_terms, values):\n",
    "    return dict(zip(all_terms, [values for _ in all_terms]))\n",
    "\n",
    "def get_doc_frequencies(bag_of_words_corpus, all_terms):\n",
    "    documents_frequencies = create_frequency_dict(all_terms, 0)\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in list(uniq_tokens):\n",
    "            documents_frequencies[uniq_token] += 1\n",
    "\n",
    "    return documents_frequencies\n",
    "\n",
    "def get_term_frequencies(bag_of_words_corpus, terms):\n",
    "    terms_frequencies = create_frequency_dict(terms, [])\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in uniq_tokens:\n",
    "            frequency = { 'nlm_unique_id': doc['nlm_unique_id'], 'freq': doc['bag_of_words'].count(uniq_token) }\n",
    "            if terms_frequencies[uniq_token]:\n",
    "                terms_frequencies[uniq_token].append(frequency)\n",
    "            else:\n",
    "                terms_frequencies[uniq_token] = [frequency]\n",
    "\n",
    "    return terms_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_doc_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'doc_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_term_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'term_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Document Vector Lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsOTPBDgLK7"
   },
   "source": [
    "# Let's find the missing files abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XDEzBjn3erOl",
    "outputId": "fc25a4a2-02c7-47f4-c9d5-dce4972880a5"
   },
   "outputs": [],
   "source": [
    "\n",
    "for path in path_xml:\n",
    "  pubmed_dict = pp.parse_medline_xml(path)\n",
    "  df = pd.DataFrame(pubmed_dict)\n",
    "  total = len(df)\n",
    "  availableDf = df.replace(r'^\\s*$', np.nan, regex=True).count().to_dict()\n",
    "  availableDf[\"total words\"] = get_total_words_from_column(df[\"abstract\"])\n",
    "  availableDf[\"path\"] = path\n",
    "  availableDf[\"total\"] = total\n",
    "  print(availableDf)\n",
    "\n",
    "  with open('00_available_data.csv', 'a') as file_obj:\n",
    "    dw_obj = DictWriter(file_obj, fieldnames=availableDf.keys())\n",
    "    if file_obj.tell() == 0:\n",
    "      dw_obj.writeheader()\n",
    "    dw_obj.writerow(availableDf)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "  #availableDf.to_csv(\"00_\" + path.split('/')[-1] + \"_available_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "98CdAdLQgBit"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Uzjxg8B-ob4E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLM.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "4b2db14b9325300dda5929fd2eec879e69c4bcabd164786ae5ecd11cdffdfb1f"
  },
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
