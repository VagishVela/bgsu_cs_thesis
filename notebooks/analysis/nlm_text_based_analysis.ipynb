{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G54aSNwBQyyG",
    "outputId": "eaadddb8-f2c4-4da0-8c6b-0a15380010d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gitpod/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pubmed_parser as pp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "from csv import DictWriter\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ET9nFBlZHdrr"
   },
   "outputs": [],
   "source": [
    "path_xml = pp.list_xml_path('../../data/raw') # list all xml paths under directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first set of data from the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DU6Uu3ppHtVs"
   },
   "outputs": [],
   "source": [
    "pubmed_dict = pp.parse_medline_xml(path_xml[0]) # dictionary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words_from_corpus(corpus, stop_words=[], stemming=False):\n",
    "    sno_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    bag_of_words = []\n",
    "    for doc in corpus:\n",
    "        docWords = []\n",
    "        for term in nltk.word_tokenize(doc['abstract']):\n",
    "            if term.lower() not in stop_words:\n",
    "                if stemming:\n",
    "                    docWords.append(sno_stemmer.stem(term))\n",
    "                else:\n",
    "                    docWords.append(term)\n",
    "        bag_of_words.append(\n",
    "            {\n",
    "                'nlm_unique_id': doc['nlm_unique_id'],\n",
    "                'bag_of_words': docWords\n",
    "            }\n",
    "        )\n",
    "    return bag_of_words\n",
    "\n",
    "def get_all_terms_from_corpus(bag_of_words_corpus):\n",
    "    all_terms = set()\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        all_terms.update(doc['bag_of_words'])\n",
    "    return list(all_terms)\n",
    "\n",
    "# This conversion to a dataframe causes some issues as the array isn't going through properly.\n",
    "def write_dict_to_json(dict, file_name):\n",
    "    f = open('../../data/processed/' + file_name + '.json', \"w\")\n",
    "    json.dump(dict, f)\n",
    "    f.close()\n",
    "\n",
    "def read_dict_from_json(file_name):\n",
    "    with open('../../data/processed/' + file_name + '.json') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def create_frequency_dict(all_terms, values):\n",
    "    return dict(zip(all_terms, [values for _ in all_terms]))\n",
    "\n",
    "def get_doc_frequencies(bag_of_words_corpus, all_terms):\n",
    "    documents_frequencies = create_frequency_dict(all_terms, 0)\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in list(uniq_tokens):\n",
    "            documents_frequencies[uniq_token] += 1\n",
    "\n",
    "    return documents_frequencies\n",
    "\n",
    "def get_term_frequencies(bag_of_words_corpus, terms):\n",
    "    terms_frequencies = create_frequency_dict(terms, [])\n",
    "\n",
    "    for doc in bag_of_words_corpus:\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        for uniq_token in uniq_tokens:\n",
    "            frequency = { 'nlm_unique_id': doc['nlm_unique_id'], 'freq': doc['bag_of_words'].count(uniq_token) }\n",
    "            if terms_frequencies[uniq_token]:\n",
    "                terms_frequencies[uniq_token].append(frequency)\n",
    "            else:\n",
    "                terms_frequencies[uniq_token] = [frequency]\n",
    "\n",
    "    return terms_frequencies\n",
    "\n",
    "# Calculate the inverse document frequency\n",
    "def get_idf(df, total_document_count):\n",
    "    return math.log10(total_document_count/df)\n",
    "\n",
    "# Calculate the tf-idf weighting\n",
    "def get_tf_idf(tf, idf):\n",
    "    return tf * idf\n",
    "\n",
    "def get_doc_vector_lengths(abstract_bag_of_words, doc_frequencies, term_frequencies):\n",
    "    vectorDocLengths = dict(zip([_['nlm_unique_id'] for _ in abstract_bag_of_words], [0 for _ in abstract_bag_of_words]))\n",
    "    totalCorpusDocCount = len(vectorDocLengths)\n",
    "\n",
    "    for doc in abstract_bag_of_words:\n",
    "        selected_doc_id = doc['nlm_unique_id']\n",
    "        uniq_tokens = set(doc['bag_of_words'])\n",
    "        totalWf2 = 0\n",
    "        for uniq_token in list(uniq_tokens):\n",
    "            idf = get_idf(doc_frequencies[uniq_token], totalCorpusDocCount)\n",
    "            tf = [termFreq for termFreq in term_frequencies[uniq_token] if termFreq['nlm_unique_id'] == selected_doc_id][0]['freq']\n",
    "\n",
    "            weightingScheme = get_tf_idf(tf, idf)\n",
    "            wf2 = weightingScheme ** 2\n",
    "            totalWf2 += wf2\n",
    "\n",
    "        # Add document vector length to it's document\n",
    "        vectorDocLengths[selected_doc_id] = math.sqrt(totalWf2)\n",
    "\n",
    "    print (\"Generated document vector lengths for \" + str(len(vectorDocLengths)) + \" documents\")\n",
    "\n",
    "    return vectorDocLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis and output to json files as data is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_bag_of_words_from_corpus(pubmed_dict), 'abstract_bag_of_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_all_terms_from_corpus(read_dict_from_json('abstract_bag_of_words')), 'all_terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_doc_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'doc_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(get_term_frequencies(read_dict_from_json('abstract_bag_of_words'), read_dict_from_json('all_terms')), 'term_frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated document vector lengths for 2408 documents\n"
     ]
    }
   ],
   "source": [
    "write_dict_to_json(\n",
    "    get_doc_vector_lengths(\n",
    "        read_dict_from_json('abstract_bag_of_words'), \n",
    "        read_dict_from_json('doc_frequencies'),\n",
    "        read_dict_from_json('term_frequencies')\n",
    "    ), \n",
    "    'doc_vector_lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is what the equivalent inputs are in our case\n",
    "# term_frequencies, doc_frequencies, abstract_bag_of_words, query, doc_vector_lengths, false\n",
    "def cosineScore(tfPostingList, dfPostingList, terms, query, docVectorLengths, alternativeWeighting=False):\n",
    "    scores = dict(zip([_['nlm_unique_id'] for _ in terms], [0 for _ in terms]))\n",
    "    totalCorpusDocCount = len(scores)\n",
    "\n",
    "    modifiedQuery = [{'nlm_unique_id': 0, 'doc': query}]\n",
    "    modifiedQueryTerms = get_bag_of_words_from_corpus(modifiedQuery)\n",
    "    tf_tqs = get_term_frequencies(modifiedQuery, modifiedQueryTerms)\n",
    "\n",
    "    for queryTerm in list(modifiedQueryTerms):\n",
    "        if queryTerm in terms:\n",
    "            tf_tq = tf_tqs[queryTerm][0]['freq']\n",
    "            if alternativeWeighting:\n",
    "                w_tq = tf_tq\n",
    "            else:\n",
    "                w_tq = getTfIdf(tf_tq, getIdf(dfPostingList[queryTerm], totalCorpusDocCount))\n",
    "            for termDoc in tfPostingList[queryTerm]:\n",
    "                if alternativeWeighting:\n",
    "                    wf_td = termDoc['freq']\n",
    "                else:\n",
    "                    wf_td = getTfIdf(termDoc['freq'], getIdf(dfPostingList[queryTerm], totalCorpusDocCount))\n",
    "                scores[termDoc['nlm_unique_id']] += wf_td * w_tq\n",
    "\n",
    "    for documentID in list(scores.keys()):\n",
    "        scores[documentID] = scores[documentID] / docVectorLengths[documentID]\n",
    "\n",
    "    sorted_score = dict(sorted(scores.items(), key=lambda elem: elem[1], reverse=True))\n",
    "\n",
    "    return sorted_score\n",
    "\n",
    "# Below is what the equivalent inputs are in our case\n",
    "# term_frequencies, doc_frequencies, abstract_bag_of_words, query, doc_vector_lengths, false\n",
    "def getDocumentResultsForQueries(tfPostingList, dfPostingList, terms, queries, docVectorLengths, alternativeWeighting=False):\n",
    "    documentResultsForQueries = []\n",
    "    for query in queries:\n",
    "        queryCosineScore = cosineScore(tfPostingList, dfPostingList, terms, query['query'], docVectorLengths, alternativeWeighting)\n",
    "        documentIDs = list(queryCosineScore.keys())\n",
    "\n",
    "        queryDocIDs = {\n",
    "            'queryID': query['queryID'],\n",
    "            'docIDs': documentIDs\n",
    "        }\n",
    "        documentResultsForQueries.append(queryDocIDs)\n",
    "\n",
    "        # Use for debugging or seeing the actual query results\n",
    "        # writeTopKResultsToFiles(query, queryCosineScore)\n",
    "\n",
    "    return documentResultsForQueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m getDocumentResultsForQueries(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     read_dict_from_json(\u001b[39m'\u001b[39;49m\u001b[39mdoc_frequencies\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     read_dict_from_json(\u001b[39m'\u001b[39;49m\u001b[39mterm_frequencies\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     read_dict_from_json(\u001b[39m'\u001b[39;49m\u001b[39mabstract_bag_of_words\u001b[39;49m\u001b[39m'\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     [{\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mradiotherapy\u001b[39;49m\u001b[39m\"\u001b[39;49m}],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     read_dict_from_json(\u001b[39m'\u001b[39;49m\u001b[39mdoc_vector_lengths\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[1;32m/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb Cell 16\u001b[0m in \u001b[0;36mgetDocumentResultsForQueries\u001b[0;34m(tfPostingList, dfPostingList, terms, queries, docVectorLengths, alternativeWeighting)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m documentResultsForQueries \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     queryCosineScore \u001b[39m=\u001b[39m cosineScore(tfPostingList, dfPostingList, terms, query[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m], docVectorLengths, alternativeWeighting)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     documentIDs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(queryCosineScore\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     queryDocIDs \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mqueryID\u001b[39m\u001b[39m'\u001b[39m: query[\u001b[39m'\u001b[39m\u001b[39mqueryID\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdocIDs\u001b[39m\u001b[39m'\u001b[39m: documentIDs\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     }\n",
      "\u001b[1;32m/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb Cell 16\u001b[0m in \u001b[0;36mcosineScore\u001b[0;34m(tfPostingList, dfPostingList, terms, query, docVectorLengths, alternativeWeighting)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m totalCorpusDocCount \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(scores)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m modifiedQuery \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mnlm_unique_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdoc\u001b[39m\u001b[39m'\u001b[39m: query}]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m modifiedQueryTerms \u001b[39m=\u001b[39m get_bag_of_words_from_corpus(modifiedQuery)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m tf_tqs \u001b[39m=\u001b[39m get_term_frequencies(modifiedQuery, modifiedQueryTerms)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m queryTerm \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(modifiedQueryTerms):\n",
      "\u001b[1;32m/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb Cell 16\u001b[0m in \u001b[0;36mget_bag_of_words_from_corpus\u001b[0;34m(corpus, stop_words, stemming)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     docWords \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(doc[\u001b[39m'\u001b[39;49m\u001b[39mabstract\u001b[39;49m\u001b[39m'\u001b[39;49m]):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mif\u001b[39;00m term\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b2275736572223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a222c22686f73744e616d65223a2276616769736876656c612d6267737563737468657369732d7332717935766c3837637a2e7373682e77732d757336342e676974706f642e696f227d/workspace/bgsu_cs_thesis/notebooks/analysis/nlm_text_based_analysis.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39mif\u001b[39;00m stemming:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract'"
     ]
    }
   ],
   "source": [
    "\n",
    "getDocumentResultsForQueries(\n",
    "    read_dict_from_json('doc_frequencies'),\n",
    "    read_dict_from_json('term_frequencies'),\n",
    "    read_dict_from_json('abstract_bag_of_words'), \n",
    "    [{\"query\": \"radiotherapy\"}],\n",
    "    read_dict_from_json('doc_vector_lengths')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
